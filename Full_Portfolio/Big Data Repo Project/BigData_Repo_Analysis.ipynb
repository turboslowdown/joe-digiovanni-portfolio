{"cells": [{"cell_type": "code", "execution_count": 1, "id": "8aa143d3-ab56-4f62-97f5-25512c62171b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Loading commits dataset...\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "commits dataset loaded successfully.\nLoading contents dataset...\ncontents dataset loaded successfully.\nLoading files dataset...\nfiles dataset loaded successfully.\nLoading languages dataset...\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "languages dataset loaded successfully.\nLoading licenses dataset...\nlicenses dataset loaded successfully.\n"}], "source": "'''\nruns!\n'''\n\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import BooleanType, StringType\nimport os\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"GitHub Analysis\").getOrCreate()\n\n# Define paths for each dataset\ndata_paths = {\n    \"commits\": \"gs://msca-bdp-data-open/final_project_git/commits\",\n    \"contents\": \"gs://msca-bdp-data-open/final_project_git/contents\",\n    \"files\": \"gs://msca-bdp-data-open/final_project_git/files\",\n    \"languages\": \"gs://msca-bdp-data-open/final_project_git/languages\",\n    \"licenses\": \"gs://msca-bdp-data-open/final_project_git/licenses\",\n}\n\n# Dictionary to store loaded dataframes\ndataframes = {}\n\n# Define an output directory to save analysis results\noutput_dir = \"github_data_output\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Function to load data with sampling\ndef load_data(dataset, path, sample_fraction=0.1):\n    try:\n        print(f\"Loading {dataset} dataset...\")\n        df = spark.read.parquet(path)\n\n        # Specific sampling for certain datasets\n        if dataset == \"languages\" or dataset == \"licenses\":\n            df = df.sample(fraction=sample_fraction * 10, seed = 42)\n        else:\n            df = df.sample(fraction=sample_fraction, seed=42)\n\n        dataframes[dataset] = df\n        print(f\"{dataset} dataset loaded successfully.\")\n    except Exception as e:\n        print(f\"Error loading {dataset} dataset: {e}\")\n\n# Load all datasets\nfor dataset, path in data_paths.items():\n    load_data(dataset, path)"}, {"cell_type": "code", "execution_count": 2, "id": "21f8a02a-2692-41fb-b561-0f9151b5dd75", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Cleaning commits dataset...\nCleaning contents dataset...\nCleaning files dataset...\nData cleaning completed\n"}], "source": "'''\nruns!\n'''\n\n# --- Data Cleaning ---\ndef clean_commits(df):\n    if df is None: return None\n    print(\"Cleaning commits dataset...\")\n    df = df.filter(F.col(\"author.time_sec\") > 0)\n    df = df.filter(~F.exists(F.col(\"difference\"), lambda x: (x.old_path.isNull() & x.new_path.isNotNull()) | (x.old_path.isNotNull() & x.new_path.isNull())))\n    df = df.drop(\"trailer\", \"difference\", \"difference_truncated\", \"encoding\")\n    df = df.dropDuplicates([\"commit\"])\n    return df\n\ndef clean_contents(df):\n    if df is None: return None\n    print(\"Cleaning contents dataset...\")\n    df = df.filter(F.col(\"id\").isNotNull() & F.col(\"size\").isNotNull())\n    df = df.withColumn(\"copies_int\", F.col(\"copies\").cast(\"int\"))\n    df = df.filter(F.col(\"copies_int\") > 0).drop(\"copies_int\")\n    df = df.withColumn(\"content\", F.when(F.col(\"binary\") == True, F.lit(None)).otherwise(F.col(\"content\")))\n    df = df.dropDuplicates([\"id\"])\n    df = df.withColumn(\"binary\", F.col(\"binary\").cast(\"boolean\"))\n    return df\n\ndef clean_files(df):\n    if df is None: return None\n    print(\"Cleaning files dataset...\")\n    df = df.filter(F.col(\"repo_name\").isNotNull() & F.col(\"path\").isNotNull() & F.col(\"id\").isNotNull())\n    df = df.filter(F.col(\"mode\") > 0).filter(F.length(F.col(\"path\")) > 0)\n    df = df.dropDuplicates([\"id\"])\n    df = df.filter(F.col(\"mode\").cast(\"string\").rlike(\"^[0-9]+$\"))\n    df = df.withColumn(\"mode\", F.col(\"mode\").cast(\"long\"))\n    return df\n\n\nif \"commits\" in dataframes:\n    dataframes[\"commits\"] = clean_commits(dataframes[\"commits\"])\nif \"contents\" in dataframes:\n    dataframes[\"contents\"] = clean_contents(dataframes[\"contents\"])\nif \"files\" in dataframes:\n    dataframes[\"files\"] = clean_files(dataframes[\"files\"])\n\nprint(\"Data cleaning completed\")"}, {"cell_type": "code", "execution_count": null, "id": "171f2c09-b39b-4037-acc9-3731e09cbefb", "metadata": {}, "outputs": [], "source": "'''\nruns!\n'''\n\n\n# --- Timeline Analysis ---\n\ndef analyze_timeline():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset missing. Skipping timeline analysis\")\n        return None\n    try:\n        print(\"Starting timeline analysis...\")\n        timeline_df = dataframes[\"commits\"].select(F.from_unixtime(F.col(\"author.time_sec\")).alias(\"commit_timestamp\"))\n        yearly_commits = timeline_df.filter(F.year(\"commit_timestamp\") >= 2008).filter(F.year(\"commit_timestamp\") <= 2022).groupBy(F.year(\"commit_timestamp\").alias(\"year\")).agg(F.count(\"*\").alias(\"commit_count\")).orderBy(\"year\")\n        monthly_commits_2016 = timeline_df.filter(F.year(\"commit_timestamp\") == 2016).groupBy(F.month(\"commit_timestamp\").alias(\"month\")).agg(F.count(\"*\").alias(\"commit_count\")).orderBy(\"month\")\n       \n        # Convert to Pandas for plotting\n        pandas_yearly_commits = yearly_commits.withColumn(\"year\", F.col(\"year\").cast(\"string\")).toPandas()\n        pandas_monthly_commits_2016 = monthly_commits_2016.withColumn(\"month\", F.col(\"month\").cast(\"string\")).toPandas()\n        \n        # Plotting\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_yearly_commits['year'], pandas_yearly_commits['commit_count'], color = 'skyblue', label = \"Yearly Commits\")\n        plt.plot(pandas_yearly_commits['year'], pandas_yearly_commits['commit_count'], \"r-\", label = \"Trendline\")\n        plt.xlabel('Year', fontsize = 12)\n        plt.ylabel('Number of Commits', fontsize = 12)\n        plt.title(\"Commits Over Time (2008 - 2022)\", fontsize = 14)\n        plt.xticks(rotation=45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_monthly_commits_2016['month'], pandas_monthly_commits_2016['commit_count'], color = 'skyblue', label = \"Monthly Commits (2016)\")\n        plt.plot(pandas_monthly_commits_2016['month'], pandas_monthly_commits_2016['commit_count'], \"r-\", label = \"Trendline\")\n        plt.xlabel('Month', fontsize = 12)\n        plt.ylabel('Number of Commits', fontsize = 12)\n        plt.title(\"Commits in 2016 by Month\", fontsize = 14)\n        plt.xticks(rotation=45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n\n        print(\"Timeline analysis completed\")\n    except Exception as e:\n        print(f\"Error in timeline analysis: {e}\")\n\nanalyze_timeline()\n"}, {"cell_type": "code", "execution_count": null, "id": "bda931fe-5931-45d9-ac2a-8b967be86a91", "metadata": {}, "outputs": [], "source": "'''\nruns!\n'''\n# --- Language Trends ---\ndef analyze_language_trends():\n    if \"languages\" not in dataframes or dataframes[\"languages\"] is None:\n        print(\"Languages dataset missing. Skipping language analysis.\")\n        return None\n    try:\n        print(\"Starting language trends analysis...\")\n        languages_df = dataframes[\"languages\"]\n        exploded_languages_df = languages_df.select(F.col(\"repo_name\"), F.explode(\"language\").alias(\"language_info\"))\n        language_summary_df = exploded_languages_df.select(F.col(\"repo_name\"), F.col(\"language_info.name\").alias(\"language\"), F.col(\"language_info.bytes\").alias(\"bytes\"))\n        total_bytes_per_language = language_summary_df.groupBy(\"language\").agg(F.sum(\"bytes\").alias(\"total_bytes\")).orderBy(F.desc(\"total_bytes\")).limit(10)\n        language_distribution_df = language_summary_df.groupBy(\"language\").agg(F.countDistinct(\"repo_name\").alias(\"repo_count\")).orderBy(F.desc(\"repo_count\")).limit(10)\n\n        # Convert to Pandas for plotting\n        pandas_language_summary = total_bytes_per_language.toPandas()\n        pandas_language_distribution = language_distribution_df.toPandas()\n\n        # Plotting\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_language_summary['language'], pandas_language_summary['total_bytes'])\n        plt.xlabel(\"Programming Language\")\n        plt.ylabel(\"Total Bytes\")\n        plt.title(\"Top 10 Languages by Total Bytes\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.show()\n\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_language_distribution['language'], pandas_language_distribution['repo_count'])\n        plt.xlabel(\"Programming Language\")\n        plt.ylabel(\"Number of Repos\")\n        plt.title(\"Top 10 Languages by Distribution\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.show()\n\n        print(\"Language analysis completed\")\n    except Exception as e:\n        print(f\"Error in language analysis: {e}\")\n\nanalyze_language_trends()"}, {"cell_type": "code", "execution_count": null, "id": "dec2cc00-25a7-475b-8c7f-ea14c949cdf5", "metadata": {}, "outputs": [], "source": "'''\nruns!\n'''\n\n# --- License Distribution ---\ndef analyze_license_distribution():\n    if \"licenses\" not in dataframes or dataframes[\"licenses\"] is None:\n        print(\"Licenses dataset missing. Skipping license analysis.\")\n        return None\n    try:\n        print(\"Starting license distribution analysis...\")\n        licenses_df = dataframes[\"licenses\"]\n        license_distribution_df = licenses_df.groupBy(\"license\").agg(F.countDistinct(\"repo_name\").alias(\"repo_count\")).orderBy(F.desc(\"repo_count\")).limit(10)\n\n       # Convert to Pandas for plotting\n        pandas_license_distribution = license_distribution_df.toPandas()\n\n        # Plotting\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_license_distribution['license'], pandas_license_distribution['repo_count'])\n        plt.xlabel(\"License\")\n        plt.ylabel(\"Number of Repositories\")\n        plt.title(\"Top 10 License Distribution\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.show()\n\n        print(\"License analysis completed.\")\n    except Exception as e:\n        print(f\"Error in license analysis: {e}\")\n\nanalyze_license_distribution()"}, {"cell_type": "code", "execution_count": null, "id": "f146bd34-3903-494b-8191-bcb7f7a38095", "metadata": {}, "outputs": [], "source": "'''\nruns! but unclear if correct\n'''\n\n# --- Repository Popularity ---\ndef analyze_repository_popularity():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset missing. Skipping repository popularity analysis.\")\n        return None\n    try:\n        print(\"Starting repository popularity analysis...\")\n        commits_df = dataframes[\"commits\"]\n        repo_popularity = commits_df.select(F.explode(\"repo_name\").alias(\"repo\")).groupBy(\"repo\").agg(F.count(\"*\").alias(\"commit_count\")).orderBy(F.desc(\"commit_count\")).limit(10)\n\n       # Convert to Pandas for plotting\n        pandas_repo_popularity = repo_popularity.toPandas()\n        \n        # Plotting\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_repo_popularity['repo'], pandas_repo_popularity['commit_count'])\n        plt.xlabel(\"Repository\")\n        plt.ylabel(\"Number of Commits\")\n        plt.title(\"Top 10 Most Popular Repositories\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.show()\n\n        print(\"Repository popularity analysis completed.\")\n    except Exception as e:\n        print(f\"Error in repository popularity analysis: {e}\")\n\nanalyze_repository_popularity()"}, {"cell_type": "code", "execution_count": null, "id": "e9b5d555-e9b7-40ac-968d-50d397650a12", "metadata": {}, "outputs": [], "source": "# --- DS/AI Technology Tracking ---\n\n'''\nruns!\n'''\n\n\ndef track_ds_ai_technologies():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset missing. Skipping DS/AI tracking\")\n        return None\n    try:\n        print(\"Starting DS/AI technology tracking...\")\n        ds_ai_keywords = [\"machine learning\", \"data science\", \"ai\", \"neural network\", \"deep learning\", \"tensorflow\", \"pytorch\", \"scikit-learn\"]\n        \n        def has_ds_ai_tech(repo_name, message):\n            repo_name = repo_name.lower()\n            message = message.lower()\n            return any(keyword in repo_name or keyword in message for keyword in ds_ai_keywords)\n\n        has_ds_ai_tech_udf = F.udf(has_ds_ai_tech, BooleanType())\n\n        commits_df = dataframes[\"commits\"]\n\n        ds_ai_repos = commits_df.select(F.explode(\"repo_name\").alias(\"repo\"), \"message\", F.year(F.from_unixtime(\"author.time_sec\")).alias(\"year\"))\n        ds_ai_repos = ds_ai_repos.sample(0.1, seed = 42) #Aggressive sample\n        ds_ai_repos = ds_ai_repos.filter(has_ds_ai_tech_udf(F.col(\"repo\"), F.col(\"message\"))).groupBy(\"year\").agg(F.count(\"*\").alias(\"unique_ds_ai_repos\")).orderBy(\"year\")\n        ds_ai_repos = ds_ai_repos.filter(F.col(\"year\") >= 2008).filter(F.col(\"year\") <= 2022)\n       \n       # Convert to Pandas for plotting\n        pandas_ds_ai_repos = ds_ai_repos.withColumn(\"year\", F.col(\"year\").cast(\"string\")).toPandas()\n        \n        # Plotting\n        plt.figure(figsize=(12, 6))\n        plt.bar(pandas_ds_ai_repos['year'], pandas_ds_ai_repos['unique_ds_ai_repos'], color = 'skyblue', label = \"DS/AI Repos\")\n        plt.plot(pandas_ds_ai_repos['year'], pandas_ds_ai_repos['unique_ds_ai_repos'], \"r-\", label = \"Trendline\")\n        plt.xlabel(\"Year\", fontsize = 12)\n        plt.ylabel(\"Number of Repositories\", fontsize = 12)\n        plt.title(\"DS/AI Repository Growth Over Time (2008 - 2022)\", fontsize = 14)\n        plt.xticks(rotation=45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"DS/AI technology tracking completed.\")\n    except Exception as e:\n        print(f\"Error in DS/AI technology tracking: {e}\")\n\ntrack_ds_ai_technologies()"}, {"cell_type": "code", "execution_count": null, "id": "5d68f4dc-81ef-48af-b89f-0726f260b564", "metadata": {}, "outputs": [], "source": "'''\ncontainer node errors\n\n'''\n\n# --- Additional Technology Analysis ---\ndef analyze_tech_popularity():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset missing. Skipping technology analysis.\")\n        return None\n\n    try:\n        print(\"Starting technology popularity analysis...\")\n        tech_keywords = [\"docker\", \"django\", \"spark\", \"redis\"]\n\n        def has_tech(repo_name, tech):\n            repo_name = repo_name.lower()\n            return tech in repo_name\n        \n        has_tech_udf = F.udf(has_tech, BooleanType())\n\n        commits_df = dataframes[\"commits\"]\n        \n        # Aggressive sampling BEFORE explode\n        sampled_commits = commits_df.sample(fraction = 0.005, seed = 42) # Even more aggressive sampling\n\n        # Overall counts\n        overall_tech_repos = sampled_commits.select(F.explode(\"repo_name\").alias(\"repo\")).sample(fraction = 0.1, seed = 42)\n        overall_counts = {tech: overall_tech_repos.filter(has_tech_udf(F.col(\"repo\"), F.lit(tech))).count() for tech in tech_keywords}\n        overall_counts_df = pd.DataFrame(list(overall_counts.items()), columns=['technology', 'repo_count'])\n\n        # AI/DS counts\n        ds_ai_keywords = [\"machine learning\", \"data science\", \"ai\", \"neural network\", \"deep learning\", \"tensorflow\", \"pytorch\"]\n        def has_ds_ai_tech(repo_name, message):\n            repo_name = repo_name.lower()\n            message = message.lower()\n            return any(keyword in repo_name or keyword in message for keyword in ds_ai_keywords)\n            \n        has_ds_ai_tech_udf = F.udf(has_ds_ai_tech, BooleanType())\n\n        ds_ai_tech_repos = sampled_commits.select(F.explode(\"repo_name\").alias(\"repo\"), \"message\").sample(fraction = 0.1, seed = 42).filter(has_ds_ai_tech_udf(F.col(\"repo\"), F.col(\"message\")))\n        \n        ds_ai_counts = {tech: ds_ai_tech_repos.filter(has_tech_udf(F.col(\"repo\"), F.lit(tech))).count() for tech in tech_keywords}\n        ds_ai_counts_df = pd.DataFrame(list(ds_ai_counts.items()), columns=['technology', 'repo_count'])\n\n\n        # AI/DS over time counts (Removed the plots, just keep the counts)\n        ds_ai_tech_overtime = sampled_commits.select(\n            \"author.time_sec\",\n             F.explode(\"repo_name\").alias(\"repo\"),\n            \"message\",\n            F.year(F.from_unixtime(\"author.time_sec\")).alias(\"year\")\n        ).filter(\n             has_ds_ai_tech_udf(F.col(\"repo\"), F.col(\"message\"))\n        ).filter(F.col(\"year\") >= 2008).filter(F.col(\"year\") <= 2022).sample(fraction = 0.1, seed = 42)\n\n        yearly_tech_counts = {}\n        for tech in tech_keywords:\n           yearly_tech_counts[tech] = ds_ai_tech_overtime.filter(has_tech_udf(F.col(\"repo\"), F.lit(tech))).groupBy(\"year\").agg(F.count(\"*\").alias(\"count\")).orderBy(\"year\").toPandas()\n           \n        \n         # Plotting\n        plt.figure(figsize=(12, 6))\n        plt.bar(overall_counts_df['technology'], overall_counts_df['repo_count'], color = 'skyblue', label = \"Total Repo Count\")\n        plt.xlabel('Technology', fontsize = 12)\n        plt.ylabel('Number of Repositories', fontsize = 12)\n        plt.title(\"Overall Technology Popularity\", fontsize = 14)\n        plt.xticks(rotation=45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n        \n        plt.figure(figsize=(12, 6))\n        plt.bar(ds_ai_counts_df['technology'], ds_ai_counts_df['repo_count'], color = 'lightcoral', label = \"DS/AI Repo Count\")\n        plt.xlabel('Technology', fontsize = 12)\n        plt.ylabel('Number of Repositories (AI/DS)', fontsize = 12)\n        plt.title(\"Technology Popularity in AI/DS Projects\", fontsize = 14)\n        plt.xticks(rotation=45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"Technology popularity analysis completed.\")\n        return yearly_tech_counts #Return the yearly tech counts in case they are needed.\n\n    except Exception as e:\n        print(f\"Error in technology analysis: {e}\")\n\nanalyze_tech_popularity()"}, {"cell_type": "code", "execution_count": null, "id": "6618b9ec-ec16-4de1-be81-22faf61633b9", "metadata": {}, "outputs": [], "source": "# --- Commit Message Analysis ---\ndef analyze_commit_messages():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset missing. Skipping commit message analysis.\")\n        return None\n    try:\n        print(\"Starting commit message analysis...\")\n        commits_df = dataframes[\"commits\"]\n\n        def categorize_commit(message):\n            message = message.lower()\n            if \"bug\" in message or \"fix\" in message:\n                return \"Bug Fix\"\n            elif \"feature\" in message or \"add\" in message:\n                return \"New Feature\"\n            elif \"refactor\" in message or \"improve\" in message:\n                return \"Refactoring\"\n            elif \"test\" in message:\n                return \"Testing\"\n            else:\n                return \"Other\"\n        \n        categorize_udf = F.udf(categorize_commit, StringType())\n        \n        commit_types = commits_df.select(categorize_udf(F.col(\"message\")).alias(\"commit_type\")).groupBy(\"commit_type\").agg(F.count(\"*\").alias(\"type_count\")).orderBy(F.desc(\"type_count\"))\n        \n        # Convert to Pandas for plotting\n        pandas_commit_types = commit_types.toPandas()\n\n        # Plotting\n        plt.figure(figsize=(10,6))\n        plt.bar(pandas_commit_types[\"commit_type\"], pandas_commit_types[\"type_count\"])\n        plt.xlabel(\"Commit Type\")\n        plt.ylabel(\"Count\")\n        plt.title(\"Commit Types\")\n        plt.xticks(rotation=45, ha=\"right\")\n        plt.tight_layout()\n        plt.show()\n        print(\"Commit message analysis completed.\")\n    except Exception as e:\n      print(f\"Error in commit message analysis: {e}\")\n\n\nanalyze_commit_messages()"}, {"cell_type": "code", "execution_count": null, "id": "c88a3f0a-0f11-44d1-a10a-62624445ad84", "metadata": {}, "outputs": [], "source": "def analyze_influential_committers():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n         print(\"Commits dataset missing. Skipping committer analysis.\")\n         return None\n    try:\n      print(\"Starting committer analysis...\")\n      commits_df = dataframes[\"commits\"]\n\n      # Use coalesce to replace null author names and emails with placeholder\n      top_committers = commits_df.groupBy(\n          F.coalesce(F.col(\"author.name\"), F.lit(\"Unknown Author\")).alias(\"author_name\"),\n          F.coalesce(F.col(\"author.email\"), F.lit(\"Unknown Email\")).alias(\"author_email\")\n      ).agg(F.count(\"*\").alias(\"commit_count\")).orderBy(F.desc(\"commit_count\")).limit(10)\n      \n       # Convert to Pandas for plotting\n      pandas_top_committers = top_committers.toPandas()\n\n        # Plotting\n      plt.figure(figsize=(10,6))\n      plt.bar(pandas_top_committers[\"author_name\"], pandas_top_committers[\"commit_count\"])\n      plt.xlabel(\"Committer\")\n      plt.ylabel(\"Commit Count\")\n      plt.title(\"Top 10 Committers\")\n      plt.xticks(rotation=45, ha=\"right\")\n      plt.tight_layout()\n      plt.show()\n\n      print(\"Committer analysis completed.\")\n    except Exception as e:\n        print(f\"Error in committer analysis: {e}\")\n        \nanalyze_influential_committers()"}, {"cell_type": "code", "execution_count": null, "id": "c749818c-aae8-4acc-86d3-e6af9b583a4f", "metadata": {}, "outputs": [], "source": "'''\ncontainer killed errors\n24/12/13 23:05:59 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2380.0 in stage 34.0 (TID 13108) (hub-msca-bdp-dphub-students-jdigiovanni-sw-6n81.c.msca-bdp-students.internal executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container from a bad node: container_1734114672005_0014_01_000008 on host: hub-msca-bdp-dphub-students-jdigiovanni-sw-6n81.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2024-12-13 23:05:59.131]Container killed on request. Exit code is 143\n[2024-12-13 23:05:59.132]Container exited with a non-zero exit code 143. \n[2024-12-13 23:05:59.132]Killed by external signal\n\n\nafter a while: Approximate duplication ratio of commit messages: 0.13758643775426194\n\n\n\n\nApproximate duplication ratio of commit messages: 0.08948844463774197\nText similarity analysis completed.\n'''\n\n# --- Text Similarity Analysis ---\ndef analyze_message_similarity():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset missing. Skipping message similarity analysis.\")\n        return None\n    try:\n        print(\"Starting text similarity analysis...\")\n        commits_df = dataframes[\"commits\"]\n        \n        # Sample the data first\n        sampled_commits_df = commits_df.sample(fraction = 0.05, seed = 42)\n\n        # Simplify: Just get a count of unique messages\n        unique_messages_count = sampled_commits_df.select(\"message\").distinct().count()\n        total_messages_count = sampled_commits_df.count()\n        duplication_ratio = 1.0 - (unique_messages_count / total_messages_count) if total_messages_count > 0 else 0.0\n\n\n        print(f\"Approximate duplication ratio of commit messages: {duplication_ratio}\")\n        print(\"Text similarity analysis completed.\")\n    except Exception as e:\n        print(f\"Error in text similarity analysis: {e}\")\n\nanalyze_message_similarity()"}, {"cell_type": "code", "execution_count": null, "id": "35910a7e-c488-4a47-96a1-1dde6078cd59", "metadata": {}, "outputs": [], "source": "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, MinHashLSH\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import ArrayType, DoubleType\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef analyze_message_similarity():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None:\n        print(\"Commits dataset not loaded or cleaned. Skipping text similarity analysis.\")\n        return None\n\n    try:\n        print(\"Starting text similarity analysis...\")\n        commits_df = dataframes[\"commits\"]\n\n        # Limit data for performance reasons\n        sample_commits_df = commits_df.sample(fraction=0.01, seed=42) #More aggressive sampling\n\n        # 1. Tokenization and TF-IDF\n        tokenizer = Tokenizer(inputCol=\"message\", outputCol=\"words\")\n        wordsData = tokenizer.transform(sample_commits_df)\n        hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\n        featurizedData = hashingTF.transform(wordsData)\n        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n        idfModel = idf.fit(featurizedData)\n        rescaledData = idfModel.transform(featurizedData)\n\n        # Filter out zero vectors\n        filteredData = rescaledData.filter(F.size(F.col(\"features.values\")) > 0)\n        \n        # Extract the values array as a new column\n        filteredData = filteredData.withColumn(\"features_vector\", F.col(\"features.values\"))\n\n        # 2. LSH\n        mh = MinHashLSH(inputCol=\"features_vector\", outputCol=\"hashes\", numHashTables=5)\n        model = mh.fit(filteredData)\n        hashedData = model.transform(filteredData)\n        \n        # 3. Self Join to get similar messages\n        hashedData.createOrReplaceTempView(\"hashed_messages\")\n        \n        similar_messages = spark.sql(\"\"\"\n            SELECT\n                m1.message AS message1,\n                m2.message AS message2,\n                m1.hashes,\n                m2.hashes\n            FROM hashed_messages m1\n            CROSS JOIN hashed_messages m2\n            WHERE m1.message != m2.message\n        \"\"\")\n        \n        # Calculate the jaccard distance\n        similar_messages = model.approxSimilarityJoin(hashedData, hashedData, 0.5, distCol=\"similarity\").select(\n            F.col(\"datasetA.message\").alias(\"message1\"),\n            F.col(\"datasetB.message\").alias(\"message2\"),\n             F.col(\"similarity\").alias(\"similarity\")\n         )\n\n        # Convert to Pandas DataFrame and plot distribution\n        pandas_similar_messages = similar_messages.toPandas()\n\n        plt.figure(figsize=(10,6))\n        plt.hist(pandas_similar_messages['similarity'], bins = 20, edgecolor='black')\n        plt.xlabel(\"Similarity Score\", fontsize = 12)\n        plt.ylabel(\"Frequency\", fontsize = 12)\n        plt.title(\"Distribution of Jaccard Similarities\", fontsize = 14)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n\n        print(f\"Message similarities analysis completed\")\n        return similar_messages\n    except Exception as e:\n        print(f\"Error in text similarity analysis: {e}\")\n        return None\n\n# Run the analysis\nsimilar_messages = analyze_message_similarity()"}, {"cell_type": "code", "execution_count": null, "id": "1e818b6a-897e-437a-8514-27961982f2e1", "metadata": {}, "outputs": [], "source": "def analyze_language_trends_over_time():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None or \"languages\" not in dataframes or dataframes[\"languages\"] is None:\n        print(\"Commits or Languages dataset missing. Skipping language trends over time analysis.\")\n    try:\n        print(\"Starting language trends over time analysis...\")\n\n        commits_df = dataframes[\"commits\"]\n        languages_df = dataframes[\"languages\"]\n\n        # Sample commits and languages dataframes\n        sampled_commits = commits_df.sample(fraction = 0.005, seed = 42) # Aggressive sampling\n        sampled_languages = languages_df.sample(fraction = 0.1, seed = 42)\n\n        # Explode language data\n        exploded_languages = sampled_languages.select(\"repo_name\", F.explode(\"language\").alias(\"language_info\"))\n        language_summary = exploded_languages.select(\"repo_name\", F.col(\"language_info.name\").alias(\"language\"))\n\n\n        # Prepare a DataFrame with year of the commit\n        commit_year = sampled_commits.select(F.explode(\"repo_name\").alias(\"repo\"), F.year(F.from_unixtime(\"author.time_sec\")).alias(\"year\"))\n\n\n        # Join commits with language data on repo name\n        joined_df = commit_year.join(language_summary, commit_year.repo == language_summary.repo_name, \"inner\")\n\n        # Group by language and year, count number of repos\n        language_over_time = joined_df.groupBy(\"language\", \"year\").agg(F.countDistinct(\"repo\").alias(\"repo_count\")).orderBy(\"year\", \"language\")\n        \n        pandas_language_over_time = language_over_time.toPandas()\n        \n        # Get the top languages to track\n        top_languages = language_summary.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).orderBy(F.desc(\"count\")).limit(5).toPandas()\n        top_lang_list = top_languages[\"language\"].tolist()\n\n         # Plotting\n        plt.figure(figsize=(12, 6))\n        for language in top_lang_list:\n            data = pandas_language_over_time[pandas_language_over_time[\"language\"] == language]\n            plt.plot(data[\"year\"].astype(str), data[\"repo_count\"], marker = \"o\", linestyle = \"-\", label = language)\n        \n        plt.xlabel(\"Year\", fontsize = 12)\n        plt.ylabel(\"Number of Repositories\", fontsize = 12)\n        plt.title(\"Language Popularity Over Time\", fontsize = 14)\n        plt.xticks(rotation=45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"Language trends over time analysis completed.\")\n    except Exception as e:\n        print(f\"Error in language trends over time analysis: {e}\")\n        \nanalyze_language_trends_over_time()"}, {"cell_type": "code", "execution_count": null, "id": "922bb4f8-b0f1-4aca-a908-4a23b2ddaac4", "metadata": {}, "outputs": [], "source": "def analyze_language_license_association():\n    if \"licenses\" not in dataframes or dataframes[\"licenses\"] is None or \"languages\" not in dataframes or dataframes[\"languages\"] is None:\n        print(\"Licenses or Languages dataset missing. Skipping language-license association analysis.\")\n        return None\n    try:\n        print(\"Starting language-license association analysis...\")\n        licenses_df = dataframes[\"licenses\"].sample(fraction = 0.1, seed = 42)\n        languages_df = dataframes[\"languages\"].sample(fraction = 0.1, seed = 42)\n\n        # Explode the language data\n        exploded_languages = languages_df.select(\"repo_name\", F.explode(\"language\").alias(\"language_info\"))\n        language_summary = exploded_languages.select(\"repo_name\", F.col(\"language_info.name\").alias(\"language\"))\n\n        # Join licenses with language data based on repo name\n        joined_df = licenses_df.join(language_summary, \"repo_name\", \"inner\")\n\n        # Count licenses per language\n        language_license_counts = joined_df.groupBy(\"language\", \"license\").agg(F.count(\"*\").alias(\"repo_count\"))\n        \n        pandas_language_license_counts = language_license_counts.toPandas()\n        \n        #Get the top languages\n        top_languages = language_summary.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).orderBy(F.desc(\"count\")).limit(5).toPandas()\n        top_lang_list = top_languages[\"language\"].tolist()\n        \n        # Plotting - grouped bar chart\n        plt.figure(figsize = (15,8))\n        bar_width = 0.15\n        \n        licenses = pandas_language_license_counts['license'].unique()\n        \n        x = range(len(top_lang_list))\n        \n        for i, lic in enumerate(licenses):\n          license_data = pandas_language_license_counts[pandas_language_license_counts['license'] == lic]\n          \n          #Create a dictionary for easy lookup\n          license_counts = {row[\"language\"]: row[\"repo_count\"] for index, row in license_data.iterrows()}\n          \n          y = [license_counts.get(lang, 0) for lang in top_lang_list]\n          \n          plt.bar([pos + (bar_width * i) for pos in x], y, width = bar_width, label = lic)\n\n        plt.xlabel('Programming Languages', fontsize = 12)\n        plt.ylabel('Number of Repositories', fontsize = 12)\n        plt.title('License Distribution per Programming Language (Top 5)', fontsize = 14)\n        plt.xticks([pos + (bar_width * (len(licenses) - 1 )/2) for pos in x], top_lang_list, rotation = 45, ha=\"right\", fontsize = 10)\n        plt.legend(fontsize = 10)\n        plt.grid(axis = 'y', linestyle = '--', alpha = 0.7)\n        plt.gca().spines['top'].set_visible(False)\n        plt.gca().spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n\n\n        print(\"Language-license association analysis completed.\")\n    except Exception as e:\n        print(f\"Error in language-license association analysis: {e}\")\n        \nanalyze_language_license_association()"}, {"cell_type": "code", "execution_count": null, "id": "a8997fc8-15ad-4478-819e-6b8ddee40992", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Starting message similarity by language analysis...\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 19:==================================>                (2378 + 46) / 3484]\r"}], "source": "def analyze_message_similarity_by_language():\n    if \"commits\" not in dataframes or dataframes[\"commits\"] is None or \"languages\" not in dataframes or dataframes[\"languages\"] is None:\n        print(\"Commits or Languages dataset missing. Skipping message similarity by language analysis.\")\n        return None\n    try:\n        print(\"Starting message similarity by language analysis...\")\n\n        commits_df = dataframes[\"commits\"]\n        languages_df = dataframes[\"languages\"]\n\n        # Sample dataframes\n        sampled_commits = commits_df.sample(fraction = 0.005, seed = 42)\n        sampled_languages = languages_df.sample(fraction = 0.1, seed = 42)\n\n\n        # Explode language data\n        exploded_languages = sampled_languages.select(\"repo_name\", F.explode(\"language\").alias(\"language_info\"))\n        language_summary = exploded_languages.select(\"repo_name\", F.col(\"language_info.name\").alias(\"language\"))\n\n        # Prepare commit dataframe with repo and year\n        commit_repo_year = sampled_commits.select(F.explode(\"repo_name\").alias(\"repo\"), \"message\")\n\n\n        # Join commit and language on the repo name\n        joined_df = commit_repo_year.join(language_summary, commit_repo_year.repo == language_summary.repo_name, \"inner\")\n\n        # Get top 5 programming languages\n        top_languages = language_summary.groupBy(\"language\").agg(F.count(\"*\").alias(\"count\")).orderBy(F.desc(\"count\")).limit(5).toPandas()\n        top_lang_list = top_languages[\"language\"].tolist()\n\n        # Create a dictionary to store duplication ratios for each language\n        duplication_ratios = {}\n        \n        # Calculate duplication ratio for each of the top 5 languages\n        for language in top_lang_list:\n            lang_df = joined_df.filter(F.col(\"language\") == language)\n            \n             # Simplify: Just get a count of unique messages\n            unique_messages_count = lang_df.select(\"message\").distinct().count()\n            total_messages_count = lang_df.count()\n            duplication_ratio = 1.0 - (unique_messages_count / total_messages_count) if total_messages_count > 0 else 0.0\n            \n            duplication_ratios[language] = duplication_ratio\n\n        # Plotting\n        plt.figure(figsize=(10,6))\n        plt.bar(duplication_ratios.keys(), duplication_ratios.values())\n        plt.xlabel(\"Programming Language\")\n        plt.ylabel(\"Duplication Ratio\")\n        plt.title(\"Duplication of Commit Messages per Programming Language (Top 5)\")\n        plt.xticks(rotation=45, ha = \"right\")\n        plt.tight_layout()\n        plt.show()\n        \n        print(\"Message similarity by language analysis completed.\")\n\n    except Exception as e:\n        print(f\"Error in message similarity by language analysis: {e}\")\n        \nanalyze_message_similarity_by_language()"}, {"cell_type": "code", "execution_count": null, "id": "d52acf0a-6cc1-4ada-9ea9-1fd64b410a33", "metadata": {}, "outputs": [], "source": "# --- Logging Data Quality ---\nimport os\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import LongType, IntegerType, FloatType, DoubleType\n\nlog_dir = \"github_data_logs\"\nos.makedirs(log_dir, exist_ok=True)\n\ndef save_schema_and_count(dataset_name, schema, row_count):\n    with open(os.path.join(log_dir, f\"{dataset_name}_schema.txt\"), \"w\") as f:\n        f.write(\"Schema:\\n\")\n        f.write(schema + \"\\n\")\n        f.write(f\"Row count: {row_count}\\n\")\n\nfor dataset, df in dataframes.items():\n    if df is None:\n        print(f\"Skipping data quality checks for {dataset} because it was not loaded.\")\n        continue\n    print(f\"Processing {dataset} for data quality checks...\")\n    \n    schema_str = df._jdf.schema().treeString()\n    row_count = df.count()\n    save_schema_and_count(dataset, schema_str, row_count)\n    print(f\"Schema and row count saved for {dataset}.\")\n\n    null_counts = df.select([(df[col].isNull().cast(\"int\")).alias(col) for col in df.columns]) \\\n                    .agg(*[F.sum(col).alias(col) for col in df.columns]) \\\n                    .collect()[0].asDict()\n                    \n    with open(os.path.join(log_dir, f\"{dataset}_nulls.txt\"), \"w\") as f:\n        f.write(\"NULL Counts:\\n\")\n        f.write(str(null_counts))\n    \n    print(f\"NULL counts saved for {dataset}.\")\n        \n    numerical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, (LongType, IntegerType, FloatType, DoubleType))]\n    if numerical_cols:\n        stats = df.select(numerical_cols).describe().limit(5).toPandas()\n        print(f\"Basic statistics for {dataset} (limited to 5 rows):\\n{stats}\")\n\n    print(\"=\"*50)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}